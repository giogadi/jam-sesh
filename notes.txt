We already did pretty good work by getting this working, huh?

So what's the first thing we wanna do? Probably deprioritize UI things for now. just adding more functionality.
+ polyphony
+ more knobs

Polyphony might be a big one because all the instruments need it. OKAY let's figure this out.

Fuckkkkk we've have to redo a bunch of the stuff we did for seq-break. I wonder if we should try to copy some of that. Like the polyphonic sequences.

....or should we start talking about MIDI? NOPE NOPE NOPE NOPE NOPE

Ai yai yai I should really think about bringing in my sound code from seq-break...

goddddd I don't know. Why did it have to be such a dilemma so early?!?!?!?!

So the next thing is to look at the sequence logic and see if we can make the synth polyphonic.

Were we _ever_ doing properly polyphonic sequences? Maybe not?

Maybe _for now_ we ignore slur/legato and just get things to work? We _don't_ want something like ableton. We'll just have a little legato row, who cares.

So each beat has a list of values. Maybe a list of known max length (equal to number of voices, obv).

Oh right: gotta remember that, if we change the sequence data struct, we gotta update what we send over the network too. Let's see what that looks like?

Let's maybe test that polyphony even works? Let's initialize a 2-voice synthesizer and play 2 notes for every one on the sequencer. NICE, it really does work. That's so dope. All we need to do then is to play the map and map the plays.

Now THERE is a difficult question: how do we decide which voice to use? Maybe easier to just set the whole thing blegh

IT FUCKIN WORKS!!!!!!!!!!!!!!!! Wow, that's a big deal. Great job asshole. Let's play some video games I guess.

Now we want to get this to a place where we can submit it. What poopoo have I done? Do I need polyphony or whatever for the drum sequencer?

Ideally, we'd go through each sequence one-by-one doing roughly the same thing. Each sequence update would know about how many voices its instrument has.

Can the drum synth play more than one sound at once? I think the answer is YES!

Try to avoid hardcoding shit if possible.

What if the first connection tells the server what the initial state should be?

Or _actually_ - the SERVER should be telling the CLIENT what is going on, right? Also, we want a headless version don't we? Whatever, just get something submittable for now, okay? We can worry about the weird server shit later, I guess. Or we do something weird like codegen to make them the same. ugh gross.

Let's ignore the scrollbar conversation for now. Just assume we are only visualizing a slice.
 0 1  2 3  4 5 6  7 8  9 10 11 12
 C C# D D# E F F# G G# A A# B  C

 C D  E G  A C
 0 2  4 7  9 12

 Oh it's so easy! We just need an array of indices for the root-relative notes.

 Let's say that sequencer view just passes the "clicked index", maybe? that was model can figure out what actual note index it is.

 But view is currently partially responsible for things. let's make it unresponsible then.


 The INPUT is the original index.

 A A# B C C# D D# E F F# G G# A A# B C C# D D# E F F# G G# A A# B  C C# D D# E F F# G G# A A# B  C C# D D# E F F# G G# A A# B  C
 2 3  4 2  3 4 2  3 4  2 3  4
 C D  E C  D E C  D E  C D  E

 O: 3
 N: 1 -> 2

What is a note index?
A is our root note.

Ok NOW let's assume that the bottom note of a scale is always 0 <= scale[0] <= 11.

FORGET IT. ASSUME 0 IS ALWAYS BOTTOM, WE'LL FIX IT IN POST

Shit, well we gotta do _some_ conversion on the View side because isn't that how we decide which notes will light up the synth?

ooooooh right, the OFFSET!! remove it for now. This is very interesting indeed. need different default view offsets for each scale. can get it automatically by always starting on the same absolute note or octave.

omfg this is gonna be so goddamn dope holy shit. Now all we need to do is get the view to show the right thing.

We need to go from noteIx to cellIx. God wait, wtf was I doing? Do I really need to do this?

Wait a minute: what if the sequencer only/also stored for each sequence element: the index into the scale

wtf something feels really fishy here, doesn't it? Ohhhh I get it now. Yeah no idk if I want to do that.

Oh LORDDDDDDD we have a lot of work to do.

So how do we make JamModel aware of how it should convert things?

This is _seriously_ a fascinating problem, holy shit. We really went somewhere.

SUMMARY: We are trying to draw the new scale-ified sequences correctly. We might be able to do this if the ViewModel's sequence is not actually a seq of notes, but rather a seq of...... cell indices????? Maybe.

Let's keep the conversion to a minimum in view, then. send over converted shit.

We *JUST* added polyphony, bro. multiple notes per sequence element.

HOLY SHIIIIIIIT IT WORKS!!!!!! Let's submit this bitch. It's gross but it'll do for now.

So what were we doing? We just got scales working. Should we make it NETWORKED? cool ok. so we just need to send along the scale info.

But why does Rust need to know anything about the messages anyway, actually? why doesn't it just forward the json string directly?

Can we send just like one huge structure and have the function decide what to send over?

wtf is stateChange again? Is it "redraw"? wtf? Can we not afford to redraw every frame?

How do we change scale? Dropdown? Sure, whatever man.

Wait a minute: what's supposed to happen if I change the scale? Lol that's a big question isn't it?

Fuck, what does the sequencer store? frequencies or noteIx? Ok, seq stores noteIx's.

Ok, sequencer stores the chromatic (raw) index from A0.

omg. default scale isn't chromatic. what in the fuck? How did I miss this?!?!?!?!?!

I think.....we fucking did it. wowza.

_now_ I understand why I considered React. Fuck me.